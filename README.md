# Traffic-Accident-Data-Analysis
Traffic Accident Data Analysis(EDA, Apriori, Hybrid Model)
![image](https://user-images.githubusercontent.com/106146283/177912974-7d4fadb4-8781-455c-b5f1-7d09d28879de.png)

가. 싱글모델

1. 의사결정나무는 분류나무, 회귀나무로 구분

분류나무
목표변수가 이산형인 분류나무는 상위 노드에서 가지분할을 할 때 분류 변수와 분류 기준값의 선택 방법으로 카이제곱통계량의 p값, 지니 지수, 엔트로피 지수 등이 사용된다.
 카이제곱 통계량의 p값은 작을수록, 지니 지수와 엔트로피 지수는 클수록 자식노드 내 이질성이 큼을 의미한다. 따라서 이질성이 작아지는 방향으로 가지분할을 수행해야 한다.
- 지니 지수 : 지니 지수는 불순도를 측정하는 방법으로 작을수록 분류가 잘 됨을 의미한다. 

회귀나무
목표변수가 연속형인 회귀나무의 경우 분류변수와 분류 기준 값의 선택 방법으로는 F 통계량의 p값, 분산의 감소량 등을 사용한다. F 통계량은 일원배치법에서의 검정 통계량으로서 값이 클수록 오차의 변동에 비해 처리 변동이 큼을 의미하므로 값이 커지는 방향으로 가지분할을 수행하게 된다. 분산의 감소량도 값이 최대화되는 방향으로 가지분할을 수행한다.
- 정지규칙 : 더 이상 분리가 일어나지 않고 현재 마디가 최종마디가 되도록 하는 여러 규칙으로 최대 나무의 깊이, 최소 관측치 수, 카이제곱통계량, 지니 지수, 엔트로피 지수 등이 있다.

![image](https://user-images.githubusercontent.com/106146283/180670140-52e493f6-974b-4969-9f44-964da79d1a5d.png)

2. Random Forest - 랜덤포레스트

앙상블 기법이며 의사나무결정(Decision Tree)을 여러개로 훈련시킨 후 그 정확도의 평균을 구하는 기법

​
장점

일반화 및 성능 우수

파라미터 조정 용이

데이터 scale, 즉 단위의 영향을 받지 않음

Overfitting, 과적합이 잘 되지 않음 - 여러 모델의 평균을 구하기 때문

​
단점

개별 트리 분석이 어렵고 트리 분리가 복잡해 지는 경향 존재

차원이 크고 희소한 데이터는 성능 미흡(e.g., text data)

모델 성능 개선이 어려움

훈련 시 메모리 소모가 큼


3. KNN - K Nearest Neighbor

데이터를 비슷한 부류끼리 모아 분류하여 라벨링 하는 분류 지도학습의 일부이며 데이터들끼리의 2차원 평면안에서 거리를 계산하여 라벨링을 한다
*데이터들 간의 거리: 유클리디안 거리(피타고라스)를 기준으로 계산

​
장점

구현하기 쉬운 알고리즘

단순 효율

훈련 단계 빠름

수치 기반 데이터 분류 작업에서의 성능 우수

​
단점

모델 생성이 없어 특징과 클래스 관계 이해가 제한적

적절한 k선택이 필요. 분류 단계가 많으면 성능에 문제가 생긴다

데이터가 많으면 분류 단계가 느려짐


4. LightGBM

부스팅 알고리즘의 일종이며 오분류된 데이터에 가중치를 부여하여 확률을 높이는 방식의 앙상블 알고리즘으로 대용량 데이터에 특화

​
장점

학습하는데 걸리는 시간이 적다, 빠른 속도

메모리 사용량이 상대적으로 적은편이다

categorical feature들의 자동 변환과 최적 분할

GPU 학습 지원

​
단점

작은 dataset을 사용할 경우 과적합 가능성이 크다 (일반적으로 10,000개 이하의 데이터를 적다고 한다)



나. 데이터 불균형 처리 방법
1) 언더샘플링

1. 무작위추출 : 무작위로 정상 데이터를 일부만 선택

2. 유의정보 : 유의한 데이터만을 남기는 방식(알고리즘 : EasyEnsemble, BalanceCascade)

언더샘플링의 경우 데이터의 소실이 매우 크고, 때로는 중요한 정상데이터를 잃게 될 수 있다.

2) 오버샘플링

1. 무작위추출 : 무작위로 소수 데이터를 복제

2. 유의정보 : 사전에 기준을 정해서 소수 데이터를 복제

정보가 손실되지 않는다는 장점이 있으나, 복제된 관측치를 원래 데이터 세트에 추가하기 만하면 여러 유형의 관측치를 다수 추가하여 오버 피팅 (overfitting)을 초래할 수 있다.

이러한 경우 trainset의 성능은 높으나 testset의 성능은 나빠질 수 있다.

3. 합성 데이터 생성 : 소수 데이터를 단순 복제하는 것이 아니라 새로운 복제본을 만들어 낸다.

SMOTE알고리즘은 오버샘플링 기법 중 합성데이터를 생성하는 방식으로 가장 많이 사용되고 있는 모델이다.
SMOTE(synthetic minority oversampling technique)란, 합성 소수 샘플링 기술로 다수 클래스를 샘플링하고 기존 소수 샘플을 보간하여 새로운 소수 인스턴스를 합성해낸다.
일반적인 경우 성공적으로 작동하지만, 소수데이터들 사이를 보간하여 작동하기 때문에 모델링셋의 소수데이터들 사이의 특성만을 반영하고 새로운 사례의 데이터 예측엔 취약할 수 있다.

ROS or ROSE (Random Over Sampling)
랜덤 오버 샘플링 방법은 데이터 수가 작은 집단의 표본 수를 늘리기 위한 방법으로 표본을 무작위로 선택하여 반복 추출하는 방법입니다. 표본의 수가 증가하지만 표본을 반복적으로 늘린 것이라 정보의 양적 측면에서는 증가했다고 보긴 어렵습니다. 오히려 중복된 표본으로 인해 Overfitting의 문제가 발생할 가능성도 증가합니다.

다. 연관분석

1) Apriori 알고리즘
빈번하지 않은 아이템 셋은 하위 아이템셋 또한 빈번할 것이다. 즉, 빈번하지 않은 아이템셋은 하위 아이템셋 또한 빈번하지 않다.

{2,3}의 지지도가 낮다면 {0,2,3}, {1,2,3},{0,1,2,3}의 지지도도 낮을 것이다.

k개의 item 단일항목집단 생성(ont-item frequent set)
단일항목집단에서 최소 지지도(support) 이상의 항목만 선택
2에서 선택된 항목만을 대상으로 2개항목집단 생성
2개항목집단에서 최소 지지도 혹은 신뢰도 이상의 항목만 선택
위 과정을 k개의 k-item frequent set을 생성할 때까지 반복

장점

원리가 간단하여 사용자가 쉽게 이해하고 의미를 파악할 수 있음.

유의한 연관성을 갖는 구매패턴을 찾아줌

단점

데이터가 클 경우 속도가 느리고 연산량이 많음

지나치게 많은 연관상품들이 나타나는 단점. 상관관계는 있더라도 인과관계(연관성)를 파악하기에 한계가 있다.

2) FP-Growth
FP Tree라는 구조를 사용하여 Apriorlil의 속도측면의 단점을 개선한 알고리즘. 여전히 연관성 한계.

모든 거래를 확인하여, 각 아이템마다 지지도(support)를 계산, 최소 지지도 이상의 아이템만 선택
모든 거래에서 빈도가 높은 아이템 순서대로 순서를 정렬
부모노드를 중심으로 거래를 자식노드로 추가해주면서 tree를 생성
새로운 아이템이 나올 경우에는 부모노드부터 시작하고, 그렇지 않으면 기존 노드에서 확장
위 과정을 모든 거래에 반복하여 FP Tree를 만들고 최소 지지도 이상의 패턴만 추출
장점

Apriori 알고리즘보다 빠르고 2번의 탐색만 필요함.

후보 itemsets 생성할 필요 없음

단점

대용량의 데이터셋에서 메모리를 효율적으로 사용하지 않음

Apriori알고리즘에 비해 설계가 어려움

지지도 계산이 FP TREE를 만들어야 가능.
